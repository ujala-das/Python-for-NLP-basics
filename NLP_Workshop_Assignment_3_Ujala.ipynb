{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1) What are regular expressions?\n",
        "\n",
        "Regular expressions are patterns used to search, match, and manipulate strings based on specific text rules.\n",
        "\n",
        "###Pattern to extract emails containing both numbers and alphabets\n",
        "\n",
        "A regex like \\b[A-Za-z0-9._%+-]+[0-9][A-Za-z0-9._%+-]*@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b can capture emails that have at least one number and one letter before the @.\n",
        "\n",
        "####**Advantages of regular expressions**\n",
        "\n",
        "They are powerful for pattern-based text matching and cleaning without manually writing long string-handling code.\n",
        "\n",
        "####**Limitations of regular expressions**\n",
        "\n",
        "They cannot understand the meaning or context of the text, only patterns."
      ],
      "metadata": {
        "id": "lOYFX4hr2yuy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOo7I5CY16yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2bc7c81-a9ca-4341-98e6-0c1898044bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted emails: ['john123@example.com', 'alice99@testmail.org']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, my email is john123@example.com and my friend's email is alice99@testmail.org. Contact us!\"\n",
        "\n",
        "pattern = r'\\b[A-Za-z0-9._%+-]+[0-9][A-Za-z0-9._%+-]*@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(\"Extracted emails:\", emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) What is the Bag of Words (BoW) technique?\n",
        "\n",
        "Bag of Words is a method that converts text into a collection of word frequencies, ignoring grammar and word order.\n",
        "\n",
        "###How it differs from regular expressions\n",
        "\n",
        "BoW focuses on word counts for text analysis, while regex focuses on pattern matching.\n",
        "\n",
        "###Limitations of BoW\n",
        "\n",
        "It ignores word order, meaning, and context, which can cause information loss."
      ],
      "metadata": {
        "id": "9sjIdsDCMJrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love Python programming\",\n",
        "    \"Python programming is fun\",\n",
        "    \"I love NLP with Python\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-EYrJcAL6CS",
        "outputId": "e2fa835f-7a57-4f98-b1ae-1b8fa8ebbfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['fun' 'is' 'love' 'nlp' 'programming' 'python' 'with']\n",
            "Bag of Words matrix:\n",
            " [[0 0 1 0 1 1 0]\n",
            " [1 1 0 0 1 1 0]\n",
            " [0 0 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) What is TF-IDF (Term Frequency–Inverse Document Frequency)?\n",
        "\n",
        "TF-IDF is a numerical measure that highlights words that are important in a document but rare across documents.\n",
        "\n",
        "###Advantages of TF-IDF\n",
        "\n",
        "It reduces the weight of common words and emphasizes unique terms, improving text representation.\n",
        "\n",
        "###How it differs from regex and BoW\n",
        "\n",
        "Unlike regex (pattern search) and BoW (raw counts), TF-IDF considers word importance relative to all documents."
      ],
      "metadata": {
        "id": "4HXAGDChMf2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiUlAQEvMqAI",
        "outputId": "f004187a-5d04-4976-dbed-df3cfea445f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature names: ['fun' 'is' 'love' 'nlp' 'programming' 'python' 'with']\n",
            "TF-IDF matrix:\n",
            " [[0.         0.         0.61980538 0.         0.61980538 0.48133417\n",
            "  0.        ]\n",
            " [0.5844829  0.5844829  0.         0.         0.44451431 0.34520502\n",
            "  0.        ]\n",
            " [0.         0.         0.44451431 0.5844829  0.         0.34520502\n",
            "  0.5844829 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Word Embeddings\n",
        "\n",
        "What it means in plain English:\n",
        "A way to turn words into numbers that carry meaning.\n",
        "In embeddings, similar words (like “king” and “queen”) end up close to each other in a “map” of words.\n",
        "\n",
        "###Breaking the name:\n",
        "\n",
        "Word → a single term from text.\n",
        "\n",
        "Embedding → mapping it into a multi-dimensional space (think coordinates).\n",
        "\n",
        "###Why use it?\n",
        "\n",
        "Captures meaning and relationships between words.\n",
        "\n",
        "Lets computers understand similarity:\n",
        "\n",
        "“man” + “woman” → both are people.\n",
        "\n",
        "“king” - “man” + “woman” → ≈ “queen”."
      ],
      "metadata": {
        "id": "1jxpc5EIMuTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Imagine these are word embeddings (numbers that represent meaning)\n",
        "\n",
        "embeddings = {\n",
        "    \"king\": [0.9, 0.8],\n",
        "    \"queen\": [0.88, 0.82],\n",
        "    \"man\": [0.4, 0.3],\n",
        "    \"woman\": [0.42, 0.35]\n",
        "}\n",
        "\n",
        "print(\"Vector for 'king':\", embeddings[\"king\"])\n",
        "print(\"Vector for 'queen':\", embeddings[\"queen\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mZU9CzQNOTS",
        "outputId": "313125da-a151-4af7-ba7c-02180ef52597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'king': [0.9, 0.8]\n",
            "Vector for 'queen': [0.88, 0.82]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5) What are stop words and how to remove them using NLTK?\n",
        "\n",
        "Stop words are common words (like “the”, “is”, “in”) that are usually removed to focus on meaningful content."
      ],
      "metadata": {
        "id": "nPBwQ8rxNeDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"I love learning NLP in Python because it is interesting\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
        "\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8AgrO0mOcEF",
        "outputId": "cd7498de-5dc9-498e-9645-5a63118cb88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered words: ['love', 'learning', 'NLP', 'Python', 'interesting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6) What is sentence tokenization and word tokenization?\n",
        "\n",
        "Sentence tokenization splits text into sentences, while word tokenization splits text into individual words."
      ],
      "metadata": {
        "id": "t6lfnGa3Q8X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sample_text = \"NLP is fun. I love learning new techniques!\"\n",
        "\n",
        "print(\"Sentence tokenization:\", sent_tokenize(sample_text))\n",
        "print(\"Word tokenization:\", word_tokenize(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K-28eYtRZ6p",
        "outputId": "2ed3156c-5c8d-4187-f672-c05728037598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence tokenization: ['NLP is fun.', 'I love learning new techniques!']\n",
            "Word tokenization: ['NLP', 'is', 'fun', '.', 'I', 'love', 'learning', 'new', 'techniques', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}